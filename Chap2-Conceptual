1. For each of parts (a) through (d), indicate whether we would generally
expect the performance of a flexible statistical learning method to be
better or worse than an inflexible method. Justify your answer.
(a) The sample size n is extremely large, and the number of predictors
p is small.

Ans: Assuming non-linear relationship, a flexible statistical learning method will perform much better as n>>p making it less likely to overfit the data while reducing the bias by fitting more closely. For a dataset with a linear relationship between the outcome and predictor variables, the perfomance of an inflexible learning will still be better.

(b) The number of predictors p is extremely large, and the number of observations n is small.

Ans: In this case, the number of features being much greater than number of observations makes it very likely that a more flexible model will be prone to overfitting the noise thus leading to high variance on the test data. An inflexible learning method will generally perform better unless the relationship itself is highly non-linear.

(c) The relationship between the predictors and response is highly non-linear.

Ans: A flexible approach is suited better to models with higher level of complexities to fit the data more closely and achieve low bias.

(d) The variance of the error terms, i.e. σ2 = Var(e), is extremely high.

Ans: On data with high variance of error terms, a more flexible approach will be prone to overfitting the training data leading to poor perfomance on the test data. Hence, an inflexible learning method is prefereable in such a situation.


2. Explain whether each scenario is a classification or regression problem, and indicate whether we are most interested in inference or prediction.
Finally, provide n and p.
(a) We collect a set of data on the top 500 firms in the US. For each
firm we record profit, number of employees, industry and the
CEO salary. We are interested in understanding which factors
affect CEO salary.

Ans: Regression, Inference, n = 500 and p = 3

(b) We are considering launching a new product and wish to know
whether it will be a success or a failure. We collect data on 20
similar products that were previously launched. For each product
we have recorded whether it was a success or failure, price
charged for the product, marketing budget, competition price,
and ten other variables.

Ans: Classification, Prediction, n = 20 and p = 13

(c) We are interesting in predicting the % change in the US dollar in
relation to the weekly changes in the world stock markets. Hence
we collect weekly data for all of 2012. For each week we record
the % change in the dollar, the % change in the US market,
the % change in the British market, and the % change in the
German market.

Ans: Regression, Prediction, n=52 and p=3


3. We now revisit the bias-variance decomposition.
(a) Provide a sketch of typical (squared) bias, variance, training error,
test error, and Bayes (or irreducible) error curves, on a single
plot, as we go from less flexible statistical learning methods
towards more flexible approaches. The x-axis should represent
the amount of flexibility in the method, and the y-axis should
represent the values for each curve. There should be five curves.
Make sure to label each one. 
(b) Explain why each of the five curves has the shape displayed in
part (a).

Ans: *The image will be inserted soon*
1) Squared Bias - Squared bias is the error due to the difference between the average predicted value (which can be obtained by training model over different sets of data) and the right value of what we are trying to predict. As the flexibility of the model increases, we can fit more closely to the training data and as a result, the error due to bias decreases.

2) Variance - Variance is how much the predicted value differs over different iterations of model trained with unique sets of data. Error due to variance increases as the flexibility increases because the model starts fitting noise too closely and a result, performs poorly on test data.

3) Training error - Training error reduces with greater flexibility of the model as the model gets more freedom to fit the points on the training data set.

4) Test error - Test error is how well a trained model performs on an entirely unknown set of data. Initially, as the flexibility of the model increases, the squared bias error tends to reduce more sharply than the increase in variance leading to a smaller test error. However, after a certain point, any further increase in the complexity of the model to increase the variance while the bias starts reaching a level of saturation. The minima in the test error represents the point of ideal bias-variance trade-off and any further increase in flexibility causes overfitting.

5) Bayes or irreducible error - This error is irreducible due to factors present outside the dataset and is the lowest achievable error rate. Since, it is beyond the control of the model, it remains constant throughout.

4. You will now think of some real-life applications for statistical learning.
(a) Describe three real-life applications in which classification might
be useful. Describe the response, as well as the predictors. Is the
goal of each application inference or prediction? Explain your
answer.

Ans:
Identifying odors for prevention of hazardous situations
Response: Classify whether a particular gas is present or not in the atmosphere above the tolerable limit for a hazard
Predictor: Multitude of gas sensors giving information about the trace amounts of various elements in the atmosphere
Goal: Prediction

Handwriting-to-text tool
Response: Classify a letter or a group of letters from a written note
Predictor: If the image fed for analysis is of size 100 x 100 pixels, and each pixel is a variable containing the brightness information then each pixel is a predictor variable and in total there are 10,000 variables.
Goal: Prediction

Assess applicants to grad school 
Response: Will this candidate score a GPA greater than 3.5 in grad school?
Predictor: GRE scores, ranking of undergraduate college, undergraduate GPA, no. of publications
Goal: Prediction

(b) Describe three real-life applications in which regression might
be useful. Describe the response, as well as the predictors. Is the
goal of each application inference or prediction? Explain your
answer.

Ans:
Housing prices
Response: Price of a house
Predictor: Crime in neighbourhood, proximity to the airport, proximity to downtown, average income in neighbourhood, number of public schools in vicinity, dimensions
Goal: Inference

Second-hand car pricing
Response: Price of a car
Predictor: Horsepower, years owned, model year, original price, condition (0-5 from very poor to excellent)
Goal: Prediction

Electoral predictions
Response: Predict vote-share of a candidate in an election
Predictor: Demographics (Age, Race, Income), Rural vs Urban
Goal: Prediction

(c) Describe three real-life applications in which cluster analysis
might be useful.
Clustering DNA sequences, Twitter: People to Follow recommendations, Music: tracks to play recommendations

5. What are the advantages and disadvantages of a very flexible (versus
a less flexible) approach for regression or classification? Under what
circumstances might a more flexible approach be preferred to a less
flexible approach? When might a less flexible approach be preferred?

Ans:
Advantages:
A flexible model does not make explicit assumptions about the form of the model or the relationships between the predictor and response variables and in doing so is able to fit a wider variety of non-linear functions. A more flexible approach is preferable when we have have little to none intuition about the relationships between the predictor variables and the response variable and making unwarranted assumptions about the model can risk fitting the data poorly. A flexible model is also suited where the goal of the problem is prediction and the understanding the relationship between variables is not of importance.

Disadvantages:
At the same time, in cases where the goal is inference, an inflexible approach is more helpful in interpreting the model. A flexible approach requires a very large number of data set to get a reliable fitting function. On datasets with high variance, a more flexible model will overfit the data leading to poor perfomance on the training sets.

6. Describe the differences between a parametric and a non-parametric
statistical learning approach. What are the advantages of a parametric
approach to regression or classification (as opposed to a nonparametric
approach)? What are its disadvantages?

Ans: A parametric approach starts with assuming a form of the model or function, f. A non-parametric approach does not make explicit assumptions about the functional form of f, rather it seeks an estimate of f that gets as close as possible to the data points without being too rough or wiggly.

A parametric approach is advantageous in simplifying the model to a few parameters so that it can be understood. It doesn't need as many number of observations as a non-parameteric approach does to fit data well. The disadvantage of parametric approach is that if the assumed form of f is wrong, it can lead to underfitting or overfitting of data based on the flexibility of the model chosen.

7. The table below provides a training data set containing six observations,
three predictors, and one qualitative response variable.
Obs. X1 X2 X3 Y
1 0 3 0 Red
2 2 0 0 Red
3 0 1 3 Red
4 0 1 2 Green
5 −1 0 1 Green
6 1 1 1 Red
Suppose we wish to use this data set to make a prediction for Y when
X1 = X2 = X3 = 0 using K-nearest neighbors.
(a) Compute the Euclidean distance between each observation and
the test point, X1 = X2 = X3 = 0.

Ans:
Obs.   X1   X2   X3  Distance(0, 0, 0)   Y
1      0    3    0   3                   Red 
2      2    0    0   2                   Red
3      0    1    3   3.2                 Red
4      0    1    2   2.2                Green
5     -1    0    1   1.4                Green
6      1    1    1   1.7                 Red

(b) What is our prediction with K = 1? Why?

Ans:The prediction for K = 1 is Green, because #5 is the nearest neighbor for K = 1.

(c) What is our prediction with K = 3? Why?

Ans:The prediction for K = 3 is Red, because #2 (Red), #5 (Green), and #6 (Red) are the nearest neighbors for K = 3. Red is the predicted class because the majority of neighbours for K=3 are red.

(d) If the Bayes decision boundary in this problem is highly nonlinear,
then would we expect the best value for K to be large or
small? Why?
We would expect the best value of K to be small because a smaller value of K makes the model more flexible and allows the Bayes decision boundary to change more responsively to the nearest neighbour/s.
